{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import unidecode\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "import spacy\n",
    "from happytransformer import HappyTextToText\n",
    "from happytransformer import TTSettings\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fusion_csv(liste_csv):\n",
    "    dataset = pd.read_csv(liste_csv[0])\n",
    "    for ds in liste_csv[1:]:\n",
    "        dataset = pd.concat([dataset, pd.read_csv(ds)], ignore_index=True)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n17 octobre 2022\\n\\n \\t\\n\\n \\t\\n\\n \\t \\tÀ bientôt, Oscar\\t\\n\\nVoici la facture de votre course annulée.\\t\\n\\n\\n \\t\\n\\n \\t\\n\\nTotal\\t15,00 €\\t\\n\\n\\tPour compenser le désagrément causé aux chauffeurs, des frais vous sont facturés si vous annulez une course 2 minutes après qu'un chauffeur l'a acceptée. Si vous devez annuler une course, faites-le avant expiration du délai afin d'éviter de payer des frais.\\t\\n\\n \\t\\n\\nFrais d'annulation\\t15,00 €\\t\\n\\n \\t\\n\\nSous-total\\t15,00 €\\t\\n\\n \\t\\n\\nPaiements\\t \\t\\n\\n\\nRevolut Oscar ••••9968\\t\\n\\n17/10/2022 4:23\\t\\n\\n15,00 €\\t\\n\\n\\n\\n\\n\\n\\nNotre protocole de sécurité a reçu la certification AENOR.\\t\\n\\n \\t\\n\\nUberX\\n\\nCourse annulée |\\t\\n\\n\\nCommande acceptée\\t\\n\\n\\nCommande annulée\\t\\n\\n\\n\\n\\n\\n\\n \\t\\n\\n\\n \\t\\n\\n\\n \\t\\n\\n\\n\\n\\nUber B.V.\\n\\nMr. Treublaan 7\\n\\n1097 DP Amsterdam\\t\\n\\n\\t\\n\\n \\t\\n\\nSi vous pensez qu'il s'agit d'une erreur, rendez-vous sur help.uber.com ou appuyez sur AIDE dans le menu de l'application Uber. Nous étudierons le problème de plus près.\\t\\n\\n\\n\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liste_csv = glob.glob('data'+os.path.sep+'*.CSV')\n",
    "\n",
    "dataset = fusion_csv(liste_csv)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_tt = HappyTextToText(\"MARIAN\", \"Helsinki-NLP/opus-mt-fr-en\")\n",
    "args = TTSettings(min_length=2)\n",
    "\n",
    "def text_translation(text):\n",
    "    lang = detect(text)\n",
    "    if lang == \"fr\":\n",
    "        #translate\n",
    "        translated_text = happy_tt.generate_text(text, args=args)\n",
    "        print(translated_text)\n",
    "    elif lang == \"en\":\n",
    "        print(\"\") #nothing happens\n",
    "    else:\n",
    "        #classify the email as autre\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/17/2022 13:27:36 - INFO - happytransformer.happy_transformer -   Using model: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextToTextResult(text='17 October 2022 See you soon, Oscar Here is the invoice for your cancelled race. Total €15.00 To compensate drivers for the inconvenience, a fee is charged if you cancel a race 2 minutes after a driver has accepted it. If you')\n"
     ]
    }
   ],
   "source": [
    "text_translation(remove_https(dataset['Corps'][0])) #just to try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(text):\n",
    "    text = str(text)\n",
    "    text = unidecode.unidecode(text)\n",
    "    text = re.sub(r\"\"\"[.,/\"':!;\\\\]\"\"\", '', text)\n",
    "    text = re.sub(r\"\"\"[0-9]+\"\"\", '', text) #removing numbers\n",
    "    text = re.sub(r\"\"\"-\"\"\", ' ', text) #uniquement - pour les mots du style \"allez-vous\"\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = text.split(' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_address(text):\n",
    "    index = len(text)\n",
    "    text = text.replace('@', ' ')\n",
    "    text = text[:index-4] + text[index-4:].replace(\".\", ' ')\n",
    "    text = text.split(' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_words_french(text):\n",
    "    stop_words = stopwords.words('french')\n",
    "    text = [word for word in text if word not in stopwords.words('french') and len(word)>1]\n",
    "    return text\n",
    "\n",
    "def stop_words_english(text):\n",
    "    stop_words = stopwords.words('english')\n",
    "    text = [word for word in text if word not in stopwords.words('english') and len(word)>1]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(nlp, texte):\n",
    "    i = 0\n",
    "    # On regarde chaque mot dans le texte\n",
    "    # Chaque mot a le numéro i\n",
    "    for mot in texte:\n",
    "        # on va lemmatizer\n",
    "        doc = nlp(mot)\n",
    "        for token in doc:\n",
    "            texte[i] = token.lemma_.lower()\n",
    "            \n",
    "        i += 1\n",
    "    \n",
    "            \n",
    "    return texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_https(corps):\n",
    "\n",
    "    text_file = open(r'corps.txt', 'w',  encoding=\"utf-8\")\n",
    "    text_file.write(corps)\n",
    "    text_file.close()\n",
    "\n",
    "        \n",
    "    final_text_file = open(r'final_corps.txt', 'w',  encoding=\"utf-8\")\n",
    "    reading_text_file = open(r'corps.txt', 'r',  encoding=\"utf-8\")\n",
    "    for line in reading_text_file:\n",
    "        if \"http\" not in line:\n",
    "            final_text_file.write(line)\n",
    "            \n",
    "    final_text_file.close()\n",
    "    reading_text_file.close()\n",
    "\n",
    "    with open('final_corps.txt', 'r', encoding=\"utf-8\") as file:\n",
    "        return file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def columns_treatment(df):\n",
    "    df = df.iloc[:,[0,1,2,3]]\n",
    "    df = df.rename(columns={'Objet':'objet','Corps':'corps','De: (nom)':'nom', 'De: (adresse)':'adresse'})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(df):\n",
    "    \n",
    "    df = columns_treatment(df)\n",
    "    \n",
    "    # nlp_fr = spacy.load('fr_core_news_md')\n",
    "    nlp_en = spacy.load('en_core_web_md')\n",
    "    \n",
    "    is_english = False\n",
    "\n",
    "    for i in df.index:\n",
    "        \n",
    "        corps = remove_https(str(df['corps'][i]))\n",
    "\n",
    "        corps = text_cleaning(corps)\n",
    "        \n",
    "        objet = text_cleaning(df['objet'][i])\n",
    "\n",
    "        #stop words cleaning for object\n",
    "        objet = stop_words_english(objet)\n",
    "        # objet = stop_words_french(objet)\n",
    "        \n",
    "        # Lemmatization\n",
    "        # objet_fr = lemmatization(nlp_fr, objet)\n",
    "        objet_en = lemmatization(nlp_en, objet)\n",
    "        \n",
    "        # on vérifie si l'objet a été lemmatizé en anglais ou non\n",
    "        # if objet_en != objet:\n",
    "        #     is_english = True\n",
    "        # if is_english:\n",
    "        # objet = objet_en\n",
    "        corps = stop_words_english(corps)\n",
    "        corps = lemmatization(nlp_en, corps)\n",
    "        # else:\n",
    "        #     objet = objet_fr\n",
    "        #     corps = stop_words_french(corps)\n",
    "        #     corps = lemmatization(nlp_fr, corps)\n",
    "\n",
    "        df['objet'][i] = objet_en\n",
    "        df['corps'][i] = corps\n",
    "\n",
    "        df['adresse'][i] = clean_address(df['adresse'][i])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data_cleaning(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a00094f430d4e0bf31d8bcd1818e632624f2ff7a6dccccbf63d2a36832140da3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
