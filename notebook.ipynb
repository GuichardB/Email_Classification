{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bapti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import unidecode\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmail = pd.read_csv(\"data/gmail_oscar.CSV\")\n",
    "\n",
    "zimbra = pd.read_csv(\"data/zimbra_oscar.CSV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [gmail, zimbra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(df):\n",
    "\n",
    "    i = 0\n",
    "    for corps in df['Corps']:\n",
    "        text_file = open(r'corps.txt', 'w',  encoding=\"utf-8\")\n",
    "        text_file.write(corps)\n",
    "        text_file.close()\n",
    "\n",
    "        \n",
    "        final_text_file = open(r'final_corps.txt', 'w',  encoding=\"utf-8\")\n",
    "        reading_text_file = open(r'corps.txt', 'r',  encoding=\"utf-8\")\n",
    "        for line in reading_text_file:\n",
    "            if \"https\" not in line:\n",
    "                final_text_file.write(line)\n",
    "            \n",
    "        final_text_file.close()\n",
    "        reading_text_file.close()\n",
    "\n",
    "        with open('final_corps.txt', 'r', encoding=\"utf-8\") as file:\n",
    "            data = file.read()\n",
    "        \n",
    "    \n",
    "        data = unidecode.unidecode(data)\n",
    "        data = data.lower()\n",
    "        data = re.sub(r\"\"\"[.,\"'-:!;]\"\"\", '', data)\n",
    "        data = re.sub('\\s+', ' ', data)\n",
    "        data = data.split(' ')\n",
    "\n",
    "\n",
    "        # print(\"avant de retirer les stopwords : \", len(data), \" mots\")\n",
    "        stop_words = stopwords.words('french')\n",
    "        data = [word for word in data if word not in stopwords.words('french') and len(word)>1]\n",
    "        # print(\"Après avoir retiré les stopwords : \", len(data), \" mots\")\n",
    "\n",
    "        # Lemmatization\n",
    "        nlp_fr = spacy.load('fr_core_news_md')\n",
    "        nlp_en = spacy.load('en_core_web_md')\n",
    "        j = 0\n",
    "        # On regarde chaque mot dans data, qui est le corps numéro i\n",
    "        # Chaque mot a le numéro j\n",
    "        for mot in data:\n",
    "            # on va lemmatizer en anglais et en français\n",
    "            doc_fr = nlp_fr(mot)\n",
    "            doc_en = nlp_en(mot)\n",
    "            for token in doc_fr:\n",
    "                token_or = token.text\n",
    "                token_fr = token.lemma_\n",
    "            for token in doc_en:\n",
    "                token_en = token.lemma_\n",
    "            \n",
    "            # on vérifie quelle langue a été lemmatizée, ou aucune\n",
    "            if token_or != token_fr:\n",
    "                lemma = token_fr\n",
    "            elif token_or != token_en:\n",
    "                lemma = token_en\n",
    "            else:\n",
    "                lemma = token_or\n",
    "            \n",
    "            # on change le mot numéro j dans data\n",
    "            data[j] = lemma\n",
    "            j+=1\n",
    "\n",
    "\n",
    "        df[\"Corps\"][i] = data\n",
    "\n",
    "        i = i+1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 120\n"
     ]
    }
   ],
   "source": [
    "print(len(gmail),len(zimbra))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATTENTION\n",
    "\n",
    "Zimbra a 12 fois plus de mails que gmail, la lemmatization prend donc 12 fois plus de temps !\n",
    "\n",
    "33min pour moi :O\n",
    "\n",
    "Voir comment modifier pour ne pas lemmatizer tous les mots dans les 2 langues -> diviser le temps de traitement par 2\n",
    "\n",
    "Pas obligé de lancer la boucle for qui suit pour tester -> tester que sur gmail\n",
    "\n",
    "## Idée pour réduire le temps\n",
    "\n",
    "traiter l'objet et le corps dans une même fonction\n",
    "\n",
    "tester les 2 langues pour l'objet, et vérifier quelle lemmatization a modifié le + de mots -> langue de l'objet\n",
    "\n",
    "une fois qu'on connait la langue de l'objet, on peut lemmatizer le corps avec la bonne langue\n",
    "\n",
    "Ne marche pas que si l'objet et le corps ne sont pas dans la même langue ou si le corps contient les 2 langues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bapti\\AppData\\Local\\Temp\\ipykernel_10080\\1008173592.py:58: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"Corps\"][i] = data\n"
     ]
    }
   ],
   "source": [
    "for df in datasets:\n",
    "    df = text_cleaning(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [octobre, bientot, oscar, voici, facture, cour...\n",
       "1    [vouloir, repondre, audessus, ce, ligne, repon...\n",
       "2    [octobre, oscar, merci, davoir, utilise, uber,...\n",
       "3    [decouvrer, vite, selection, jusqua, demain, s...\n",
       "4    [annoncer, correspondre, recherche, location, ...\n",
       "5    [annoncer, correspondre, recherche, location, ...\n",
       "6    [erinette, repondu, avis, merci, davoir, publi...\n",
       "7    [rencontrer, problme, toucher, paiement, premi...\n",
       "8    [cher, client, cher, client, informer, recepti...\n",
       "9    [solde, bas, bonjour, oscar, solde, compte, ca...\n",
       "Name: Corps, dtype: object"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmail['Corps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "27fd6a064d6c7c8082aaa95ab329c8297122ddbb839f6b085e9f9d2309e1bfb3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
