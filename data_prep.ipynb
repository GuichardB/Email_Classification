{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation for NLP algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import unidecode\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "import spacy\n",
    "from happytransformer import HappyTextToText\n",
    "from happytransformer import TTSettings\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recovering the emails from different email boxes\n",
    "\n",
    "For this first step, we use Outlook to export in a csv file all the emails from different boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSVs fusion\n",
    "it permits to fusionnate the CSVs files coming from different emails box from Outlook, in order to get only one dataset with all the emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fusion_csv(liste_csv):\n",
    "    dataset = pd.read_csv(liste_csv[0])\n",
    "    for ds in liste_csv[1:]:\n",
    "        dataset = pd.concat([dataset, pd.read_csv(ds)], ignore_index=True)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_csv = glob.glob('data'+os.path.sep+'*.CSV')\n",
    "\n",
    "dataset = fusion_csv(liste_csv)\n",
    "\n",
    "dataset = dataset.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns treatment\n",
    "The aim here is just to select the columns we wanted to keep, and to rename them. The three columns at the end are : object, body and address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def columns_treatment(df):\n",
    "    df = df.iloc[:,[0,1,3]]\n",
    "    df = df.rename(columns={'Objet':'objet','Corps':'corps', 'De: (adresse)':'adresse'})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove https\n",
    "Doing our data exploration, we saw that there were a lot of links in the different body emails and we needed to remove them because it would have disturbed our model. So we removed it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_https(corps):\n",
    "\n",
    "    text_file = open(r'corps.txt', 'w',  encoding=\"utf-8\")\n",
    "    text_file.write(corps)\n",
    "    text_file.close()\n",
    "\n",
    "        \n",
    "    final_text_file = open(r'final_corps.txt', 'w',  encoding=\"utf-8\")\n",
    "    reading_text_file = open(r'corps.txt', 'r',  encoding=\"utf-8\")\n",
    "    for line in reading_text_file:\n",
    "        if \"http\" not in line:\n",
    "            final_text_file.write(line)\n",
    "            \n",
    "    final_text_file.close()\n",
    "    reading_text_file.close()\n",
    "\n",
    "    with open('final_corps.txt', 'r', encoding=\"utf-8\") as file:\n",
    "        return file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text cleaning\n",
    "For this part, the goal is to delete all the parasitic characters in the body and the object of an email, we used re.sub to do this operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(text):\n",
    "    text = str(text)\n",
    "    text = unidecode.unidecode(text)\n",
    "    text = re.sub(r\"\"\"[.,(/\"'?:)!;\\\\]\"\"\", '', text)\n",
    "    text = re.sub(r\"\"\"[0-9]+\"\"\", '', text) #removing numbers\n",
    "    text = re.sub(r\"\"\"-\"\"\", ' ', text) #uniquement - pour les mots du style \"allez-vous\"\n",
    "    text = re.sub(r\"\"\"_\"\"\", ' ', text) \n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = re.sub(r'\\<.*?\\>', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text translation\n",
    "We noticed, during the first attempts of data cleaning, that it was time consuming to deal with the different languages in our dataset. Moreover, we noticed that the lemmatization was quite more effective on english emails than on french email. So we decided to translate all our french emails in english, using a Neural Network model from Hugging Face, it is named : Helsinki-NLP/opus-mt-fr-en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_tt = HappyTextToText(\"MARIAN\", \"Helsinki-NLP/opus-mt-fr-en\")\n",
    "args = TTSettings(min_length=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_translation(text):\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "    except:\n",
    "        lang = \"error\"\n",
    "    if lang == \"fr\":\n",
    "        #translate\n",
    "        final_trans_text = \"\"\n",
    "        ran = round(len(text.split())/50)\n",
    "        if ran == 0:\n",
    "            trans_text = happy_tt.generate_text(text, args=args)\n",
    "            final_trans_text = trans_text.text\n",
    "        else:\n",
    "            for i in range(ran):\n",
    "                piece_of_text = ' '.join(text.split()[i*50:50+(i*50)])                \n",
    "                trans_text = happy_tt.generate_text(piece_of_text, args=args)\n",
    "                final_trans_text = final_trans_text + \" \" + trans_text.text\n",
    "        \n",
    "    elif lang == \"en\":\n",
    "        final_trans_text = text\n",
    "        print(\"\") #nothing happens\n",
    "    else:\n",
    "        #classify the email as autre\n",
    "        final_trans_text = text\n",
    "        print(\"\")\n",
    "    return final_trans_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stop words\n",
    "Some words are not usefull at all and are only noise in a text for NLP algorithms. These words correspond to linking word such as 'and' or other words like 'to' for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_words_english(text):\n",
    "    stop_words = stopwords.words('english')\n",
    "    text = [word for word in text.split() if ((word not in stop_words) and (len(word)>1))]\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "is the process of grouping together the inflected forms of a word so they can be analysed as a single item. For example, it permits to put all verbs in indicative form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(nlp, texte):\n",
    "    i = 0\n",
    "    # On regarde chaque mot dans le texte\n",
    "    # Chaque mot a le numéro i\n",
    "    for mot in texte:\n",
    "        # on va lemmatizer\n",
    "        doc = nlp(mot)\n",
    "        for token in doc:\n",
    "            texte[i] = token.lemma_.lower()\n",
    "            \n",
    "        i += 1\n",
    "    \n",
    "            \n",
    "    return texte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean adress \n",
    "In our dataset, we decided to clean the address column, keeping a list of three elements, here is the pattern : [name before '@', name after '@', the domain which is at the end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_address(text):\n",
    "    index = len(text)\n",
    "    text = text.replace('@', ' ')\n",
    "    text = text[:index-4] + text[index-4:].replace(\".\", ' ')\n",
    "    text = text.split(' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning function\n",
    "Here is the function calling all the different functions we created above, in order to retunr a cleaned dataset at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(df, nb):\n",
    "    \n",
    "    df = columns_treatment(df)\n",
    "    \n",
    "    nlp_en = spacy.load('en_core_web_md')\n",
    "\n",
    "    for i in df.index:\n",
    "        \n",
    "        corps = remove_https(str(df['corps'][i]))\n",
    "\n",
    "        corps = text_cleaning(corps)\n",
    "\n",
    "        corps = text_translation(corps)\n",
    "        \n",
    "        objet = text_cleaning(df['objet'][i])\n",
    "\n",
    "        objet = text_translation(objet)\n",
    "\n",
    "        \n",
    "        #stop words cleaning for object\n",
    "        objet = stop_words_english(objet)\n",
    "        objet_en = lemmatization(nlp_en, objet)\n",
    "        \n",
    "        #stop words cleaning for corps\n",
    "        corps = stop_words_english(corps)\n",
    "        corps = lemmatization(nlp_en, corps)\n",
    "\n",
    "        df['objet'][i] = objet_en\n",
    "        df['corps'][i] = corps\n",
    "\n",
    "        df['adresse'][i] = clean_address(df['adresse'][i])\n",
    "        print(\"dernier mail traité est le numéro : \", nb)\n",
    "        nb = nb+1\n",
    "        if nb%500 == 0:\n",
    "            df.to_csv('df_'+str(nb)+'.CSV')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the dataset and print it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data_cleaning(dataset, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a00094f430d4e0bf31d8bcd1818e632624f2ff7a6dccccbf63d2a36832140da3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
